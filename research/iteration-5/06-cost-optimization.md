# Cost Optimization: Minimizing Memory System Costs

*Research conducted: 2026-02-17*  
*Author: Jordan üß≠*  
*Status: Complete*

---

## Executive Summary

AI memory systems can become expensive at scale. The primary cost drivers are LLM API calls, embedding generation, and vector database storage. This research identifies optimization strategies ranked by impact, provides a cost model, and establishes a decision framework for cost vs. quality tradeoffs.

**Key finding:** A well-optimized memory system can reduce costs by **60-80%** compared to naive implementations while maintaining 90%+ of quality through strategic caching, model tiering, and intelligent batching.

---

## 1. Cost Model

### Cost Components

| Component | Typical % of Total | Variability | Optimization Potential |
|-----------|-------------------|-------------|----------------------|
| LLM API (extraction/retrieval) | 40-60% | High | High |
| Embedding generation | 15-25% | Medium | High |
| Vector DB storage | 10-20% | Low | Medium |
| Vector DB operations (queries) | 5-15% | Medium | Medium |
| Compute (processing) | 5-10% | Medium | Medium |
| Bandwidth | <5% | Low | Low |

### Current Pricing Reference (2026)

**LLM APIs (per 1M tokens):**
| Model | Input | Output | Cached Input | Use Case |
|-------|-------|--------|--------------|----------|
| GPT-5.2 | $1.75 | $14.00 | $0.175 | Complex extraction |
| GPT-5 mini | $0.25 | $2.00 | $0.025 | Standard extraction |
| GPT-4.1 nano | $0.20 | $0.80 | $0.05 | Simple tasks |
| Claude Opus 4.6 | $5.00 | $25.00 | $0.50 (cache read) | Complex reasoning |
| Claude Sonnet 4.5 | $3.00 | $15.00 | $0.30 (cache read) | Balanced |
| Claude Haiku 4.5 | $1.00 | $5.00 | $0.10 (cache read) | Cost-efficient |
| Claude Haiku 3 | $0.25 | $1.25 | $0.03 (cache read) | Budget option |

**Key insight:** Haiku 3 is **20x cheaper** than Opus 4.6 for input, **20x cheaper** for output. For extraction tasks that don't require frontier reasoning, this is a massive savings opportunity.

**Embedding Models (per 1M tokens):**
| Provider | Model | Price | Dimensions | Notes |
|----------|-------|-------|------------|-------|
| OpenAI | text-embedding-3-small | ~$0.016 | 1536 (adjustable) | Matryoshka support |
| OpenAI | text-embedding-3-large | ~$0.104 | 3072 (adjustable) | Higher quality |
| Voyage AI | voyage-4-lite | $0.02 | - | Budget option |
| Voyage AI | voyage-4 | $0.06 | - | Balanced |
| Voyage AI | voyage-4-large | $0.12 | - | High quality |
| Cohere | embed-v4.0 | ~$0.10 | 1024 (adjustable) | Multimodal |
| Pinecone | llama-text-embed-v2 | $0.16 | - | Integrated |
| Pinecone | multilingual-e5-large | $0.08 | - | Budget multilingual |

**Vector Database Storage:**
| Provider | Storage | Read Operations | Write Operations |
|----------|---------|-----------------|------------------|
| Pinecone (Standard) | $0.33/GB/mo | $16/M RU | $4/M WU |
| Pinecone (Enterprise) | $0.33/GB/mo | $24/M RU | $6/M WU |
| Weaviate (Flex) | $0.255/GiB | Included* | Included* |
| Qdrant Cloud | ~$0.05/GB/mo | Varies | Varies |
| Self-hosted | $0.02-0.10/GB/mo | Compute only | Compute only |

*Weaviate charges per vector dimension instead of operations

### Monthly Cost Scenarios

**Scenario A: Small deployment (1 user, casual use)**
- 100 conversations/month, ~50 memories extracted
- LLM extraction: ~$0.50-2.00
- Embeddings: ~$0.01-0.05
- Vector storage: ~$0.10
- **Total: $1-3/month**

**Scenario B: Medium deployment (10 users, daily use)**
- 3,000 conversations/month, ~1,500 memories
- LLM extraction: $15-60
- Embeddings: $0.30-1.50
- Vector storage: $1-5
- **Total: $20-70/month**

**Scenario C: Large deployment (1,000 users, heavy use)**
- 300,000 conversations/month, ~150,000 memories
- LLM extraction: $1,500-6,000
- Embeddings: $30-150
- Vector storage: $100-500
- **Total: $2,000-7,000/month**

---

## 2. Optimization Strategies (Ranked by Impact)

### Tier 1: High Impact (30-50% savings each)

#### 2.1 Model Tiering for Extraction
**Impact: 40-60% LLM cost reduction**

Not all memories require frontier models. Implement a tiered extraction system:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 Incoming Content                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ  Complexity Classifier ‚îÇ  (rules or small model)
          ‚îÇ     (nearly free)      ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ                ‚îÇ                ‚îÇ
     ‚ñº                ‚ñº                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Simple  ‚îÇ    ‚îÇ   Standard  ‚îÇ    ‚îÇ Complex  ‚îÇ
‚îÇ Haiku 3 ‚îÇ    ‚îÇ GPT-5 mini  ‚îÇ    ‚îÇ Sonnet   ‚îÇ
‚îÇ  70%    ‚îÇ    ‚îÇ    25%      ‚îÇ    ‚îÇ   5%     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Classification rules (simple heuristics):**
- **Simple (70%):** Short messages, casual conversation, basic facts
- **Standard (25%):** Multi-topic, some nuance, moderate length
- **Complex (5%):** Emotional content, contradictions, implicit preferences, corrections

**Cost comparison:**
- Naive (all Sonnet): 100 √ó $3.00 input + $15.00 output = ~$1,800/M tokens
- Tiered: 70 √ó $0.25 + 25 √ó $0.25 + 5 √ó $3.00 = ~$38.75/M input tokens
- **Savings: ~50%**

#### 2.2 Aggressive Caching
**Impact: 40-70% reduction in repeat processing**

**Three-layer cache architecture:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Layer 1: Response Cache (Redis)                    ‚îÇ
‚îÇ  - Exact query ‚Üí result mapping                     ‚îÇ
‚îÇ  - TTL: 5 minutes (hot), 1 hour (warm)              ‚îÇ
‚îÇ  - Hit rate: 30-40% for repeat patterns             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ miss
                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Layer 2: Embedding Cache (Redis/Disk)              ‚îÇ
‚îÇ  - Content hash ‚Üí embedding vector                  ‚îÇ
‚îÇ  - TTL: 24 hours to indefinite                      ‚îÇ
‚îÇ  - Hit rate: 60-80% (many duplicates/near-dupes)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ miss
                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Layer 3: Extraction Cache (DB)                     ‚îÇ
‚îÇ  - Content hash ‚Üí extracted memories                ‚îÇ
‚îÇ  - TTL: Indefinite (invalidate on model change)     ‚îÇ
‚îÇ  - Hit rate: 20-30% for re-processed content        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Cache key strategies:**
- Exact match: SHA-256 of normalized content
- Semantic dedup: Check if embedding similarity > 0.98 before processing
- Prompt caching: Use provider prompt caching (10x cheaper than fresh input)

**Provider prompt caching comparison:**
| Provider | Cache Read Cost | vs. Fresh Input |
|----------|-----------------|-----------------|
| OpenAI | 10% of input | 90% savings |
| Anthropic | 10% of input | 90% savings |

#### 2.3 Batch Processing
**Impact: 30-50% reduction via batch discounts**

Both OpenAI and Anthropic offer 50% discounts for batch processing:

**When to batch:**
- Background memory consolidation
- Periodic re-extraction (model updates)
- Bulk imports
- Night/off-peak processing

**Implementation:**
```python
# Batch API pattern
batch_jobs = []
for conversation in pending_conversations:
    batch_jobs.append({
        "custom_id": conversation.id,
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "gpt-5-mini",
            "messages": extraction_prompt(conversation)
        }
    })

# Submit batch (50% discount, 24h completion)
batch = client.batches.create(
    input_file_id=upload_batch_file(batch_jobs),
    endpoint="/v1/chat/completions",
    completion_window="24h"
)
```

**Batch eligibility analysis:**
- Real-time retrieval: Not batchable
- Memory extraction: 80% batchable (background)
- Consolidation: 100% batchable
- Re-embedding: 100% batchable

### Tier 2: Medium Impact (15-30% savings each)

#### 2.4 Matryoshka Embeddings (Dimension Reduction)
**Impact: 15-50% storage and compute reduction**

Matryoshka Representation Learning allows truncating embedding dimensions while preserving most semantic quality:

```
Full embedding (1536 dims):     [0.1, 0.2, ..., 0.8]  100% quality
Truncated (768 dims):           [0.1, 0.2, ..., 0.4]   98% quality
Truncated (256 dims):           [0.1, 0.2, 0.3, 0.4]   94% quality
```

**OpenAI text-embedding-3-small benchmarks:**
| Dimensions | MTEB Score | Storage/Vector | % of Full |
|------------|------------|----------------|-----------|
| 1536 | 62.3% | 6.1 KB | 100% |
| 512 | ~61% | 2.0 KB | 33% |
| 256 | ~59% | 1.0 KB | 16% |

**Recommendation:** Start with 512-768 dimensions. Only use full dimensions if retrieval precision issues emerge.

**Cost impact:**
- Storage: Linear reduction with dimensions
- Query cost: Often proportional to dimensions
- **Net savings: 30-50% on vector DB costs**

#### 2.5 Embedding Quantization
**Impact: 2-4x storage reduction**

Vector databases support quantized storage:

| Quantization | Bytes/Dim | Quality Loss | Use Case |
|--------------|-----------|--------------|----------|
| Float32 | 4 | None | Gold standard |
| Float16 | 2 | Negligible | Recommended default |
| Int8 | 1 | <1% | Good tradeoff |
| Binary | 0.125 | 5-10% | Coarse filtering only |

**Weaviate example:**
```yaml
# Enable Product Quantization (PQ)
vectorIndexConfig:
  pq:
    enabled: true
    segments: 128
    centroids: 256
```

**Combined with Matryoshka:**
- 1536 dims √ó Float32 = 6,144 bytes
- 512 dims √ó Int8 = 512 bytes
- **12x storage reduction**

#### 2.6 Importance-Based Processing
**Impact: 20-40% processing reduction**

Not all content deserves memory extraction. Skip low-value content:

**Skip criteria:**
- Very short messages (< 10 tokens): "ok", "thanks", "lol"
- Pure greetings/farewells
- Bot commands/system messages
- Near-duplicate content (similarity > 0.95 to recent)
- Low-engagement conversations (user left quickly)

**Importance scoring:**
```python
def calculate_importance(content, context):
    score = 0.5  # baseline
    
    # Positive signals
    if contains_personal_info(content): score += 0.2
    if user_explicitly_shared(content): score += 0.3
    if contains_preferences(content): score += 0.15
    if long_conversation(context): score += 0.1
    
    # Negative signals  
    if is_greeting(content): score -= 0.3
    if very_short(content): score -= 0.25
    if is_duplicate(content): score -= 0.4
    
    return max(0, min(1, score))

# Only process if importance > threshold
if calculate_importance(msg, ctx) > 0.4:
    extract_memories(msg)
```

**Expected skip rate:** 30-50% of messages are low-value

### Tier 3: Lower Impact (5-15% savings)

#### 2.7 Tiered Storage (Hot/Warm/Cold)
**Impact: 10-20% storage cost reduction**

Most memories are accessed infrequently:
- **Hot (recent 30 days):** 10% of memories, 80% of accesses
- **Warm (30-180 days):** 30% of memories, 15% of accesses
- **Cold (180+ days):** 60% of memories, 5% of accesses

**Implementation options:**

**Option A: Separate indexes**
```
hot_index:   Pinecone Standard, high-perf pods
warm_index:  Pinecone Standard, low-perf pods
cold_index:  Qdrant self-hosted, compressed
```

**Option B: Weaviate tenant offloading**
```python
# Move cold tenant to cheaper storage
client.collections.tenants.update(
    collection_name="Memory",
    tenants=[Tenant(name="user_123", activity_status="COLD")]
)
```

**Option C: Archive to object storage**
```python
# For truly cold data, export to S3
def archive_cold_memories(user_id, cutoff_date):
    cold_memories = query_memories(user_id, before=cutoff_date)
    s3.put_object(
        Bucket="memory-archive",
        Key=f"{user_id}/archive.jsonl",
        Body=export_jsonl(cold_memories)
    )
    delete_from_vector_db(cold_memories)
```

**Cost comparison (1M vectors):**
| Tier | Monthly Cost | Notes |
|------|--------------|-------|
| Pinecone hot | $330 | Fast, expensive |
| Pinecone warm | $33 | Restore on demand |
| S3 archive | $0.50 | Re-embed to restore |

#### 2.8 Memory Consolidation
**Impact: 10-30% storage reduction**

Over time, memories accumulate redundantly. Consolidate:

**Before consolidation:**
```
Memory 1: "User likes coffee"
Memory 2: "User mentioned enjoying coffee"  
Memory 3: "User prefers lattes"
Memory 4: "User gets coffee every morning"
```

**After consolidation:**
```
Memory (consolidated): "User is a coffee enthusiast who drinks 
lattes every morning" (confidence: 0.95, sources: 4)
```

**Consolidation triggers:**
- Memory count exceeds threshold per topic
- Periodic cleanup (weekly/monthly)
- Storage pressure

**Savings:** Typically 20-40% reduction in memory count after consolidation

#### 2.9 Regional Pricing Arbitrage
**Impact: 5-20% based on region**

Cloud pricing varies by region:

| Region | Relative Cost | Latency Trade-off |
|--------|---------------|-------------------|
| US East | 1.0x (baseline) | Lowest for US users |
| US West | 1.0x | Similar |
| Europe | 1.1-1.2x | GDPR compliance |
| Asia Pacific | 0.9-1.1x | Varies |

**Strategy:** 
- Use cheapest region for batch processing (latency doesn't matter)
- Use user-local region for real-time retrieval (latency matters)

---

## 3. Architecture Decisions

### 3.1 Serverless vs. Dedicated

| Factor | Serverless | Dedicated |
|--------|------------|-----------|
| Low volume (<1M vectors) | ‚úÖ Cost-effective | ‚ùå Over-provisioned |
| High volume (>10M vectors) | ‚ùå Expensive | ‚úÖ Cost-effective |
| Bursty traffic | ‚úÖ Scales automatically | ‚ö†Ô∏è Need headroom |
| Predictable traffic | ‚ö†Ô∏è Pay for idle | ‚úÖ Predictable cost |
| Cold starts | ‚ùå Latency spikes | ‚úÖ Always warm |

**Recommendation:**
- Start serverless (Pinecone Starter, Qdrant Cloud free tier)
- Move to dedicated at ~5-10M vectors or ~$500/mo spend

### 3.2 Self-Hosted vs. Managed

| Factor | Self-Hosted | Managed |
|--------|-------------|---------|
| Cost at scale | 30-70% cheaper | Premium for convenience |
| Operational burden | High | Minimal |
| Control | Full | Limited |
| Best for | >$2k/mo spend, strong ops team | <$2k/mo or limited ops |

**Self-hosting cost model:**
```
# Qdrant on EC2
Instance: r6g.large ($0.10/hr) = $73/mo
Storage: 500GB gp3 = $50/mo  
Total: ~$125/mo for 10M+ vectors

# vs. Pinecone managed
~$500-1000/mo for similar scale
```

**Break-even point:** ~$500/mo managed spend justifies self-hosting investment

### 3.3 Embedding Model Selection

| Priority | Recommended Model | Why |
|----------|-------------------|-----|
| Cost-first | Voyage voyage-4-lite ($0.02/M) | Cheapest quality option |
| Balanced | OpenAI text-embedding-3-small ($0.016/M) | Good quality, Matryoshka |
| Quality-first | Voyage voyage-4-large ($0.12/M) | Best retrieval quality |
| Self-hosted | sentence-transformers all-MiniLM-L6 | Free (compute only) |

**Cost comparison for 100M tokens/month:**
| Model | Monthly Cost |
|-------|--------------|
| Self-hosted | ~$10 (compute) |
| Voyage lite | $2,000 |
| OpenAI small | $1,600 |
| Voyage large | $12,000 |

---

## 4. Cost Monitoring Framework

### 4.1 Metrics to Track

**Per-user metrics:**
```sql
CREATE TABLE user_cost_metrics (
    user_id UUID,
    period DATE,
    llm_input_tokens BIGINT,
    llm_output_tokens BIGINT,
    llm_cost DECIMAL(10,4),
    embedding_tokens BIGINT,
    embedding_cost DECIMAL(10,4),
    vector_storage_bytes BIGINT,
    vector_queries INT,
    vector_cost DECIMAL(10,4),
    total_cost DECIMAL(10,4),
    PRIMARY KEY (user_id, period)
);
```

**System-wide metrics:**
```sql
CREATE TABLE system_cost_metrics (
    period DATE PRIMARY KEY,
    total_llm_cost DECIMAL(12,2),
    total_embedding_cost DECIMAL(12,2),
    total_storage_cost DECIMAL(12,2),
    total_query_cost DECIMAL(12,2),
    cache_hit_rate FLOAT,
    batch_percentage FLOAT,
    skip_rate FLOAT,
    cost_per_memory DECIMAL(10,4),
    cost_per_retrieval DECIMAL(10,6)
);
```

### 4.2 Key Performance Indicators

| KPI | Target | Alert Threshold |
|-----|--------|-----------------|
| Cost per memory extracted | <$0.01 | >$0.05 |
| Cost per retrieval | <$0.001 | >$0.005 |
| Cache hit rate | >60% | <40% |
| Batch processing rate | >50% | <30% |
| Low-value skip rate | >30% | <20% |

### 4.3 Anomaly Detection

```python
def detect_cost_anomaly(user_id, current_cost, period="daily"):
    historical = get_cost_history(user_id, lookback=30)
    mean = np.mean(historical)
    std = np.std(historical)
    
    z_score = (current_cost - mean) / std if std > 0 else 0
    
    if z_score > 3:  # 3 standard deviations
        alert_anomaly(user_id, current_cost, mean, z_score)
        
    # Also check absolute thresholds
    if current_cost > user_cost_limit(user_id):
        throttle_user(user_id)
```

### 4.4 Cost Forecasting

```python
def forecast_monthly_cost(current_usage, days_elapsed):
    daily_rate = current_usage / days_elapsed
    days_in_month = 30
    projected = daily_rate * days_in_month
    
    # Add buffer for uncertainty
    lower_bound = projected * 0.8
    upper_bound = projected * 1.3
    
    return {
        "projected": projected,
        "lower": lower_bound,
        "upper": upper_bound,
        "confidence": min(0.95, days_elapsed / 15)  # More confident with more data
    }
```

---

## 5. Cost vs. Quality Tradeoffs

### 5.1 Where to Cut Corners Safely

**Safe to optimize aggressively:**
- Embedding dimensions (512 vs 1536 = minimal quality loss)
- Skip rate for trivial messages (30-50% skip rate is fine)
- Batch processing for non-urgent operations
- Cache aggressively (stale data rarely matters for memories)
- Consolidation (fewer, richer memories > many thin ones)

**Optimize cautiously:**
- Model tiering for extraction (quality variance)
- Quantization levels (test retrieval quality)
- Storage tiering (latency impact)

**Don't optimize:**
- Retrieval quality for active conversations
- Processing of explicit user corrections
- Security/privacy controls

### 5.2 Quality Gates

```python
def should_process_with_premium_model(content, context):
    """Quality gate: some content is worth premium processing."""
    
    # Always use premium for:
    if is_user_correction(content):
        return True
    if contains_strong_emotion(content):
        return True
    if is_explicitly_shared("remember this", content):
        return True
    if user_is_premium(context.user_id):
        return True
        
    # Otherwise, use tiered selection
    return False
```

### 5.3 User-Visible Impact Matrix

| Optimization | User Impact | Acceptable? |
|--------------|-------------|-------------|
| Model tiering | May miss subtle nuances | ‚úÖ Usually |
| Skip low-value | Won't remember "ok" | ‚úÖ Yes |
| Embedding dimension | Slightly worse recall | ‚úÖ Usually |
| Batch processing | 1-24h delay | ‚úÖ For background |
| Cold storage | Slower retrieval | ‚ö†Ô∏è Test UX |
| Aggressive consolidation | Less granular history | ‚ö†Ô∏è Test UX |

---

## 6. Implementation Roadmap

### Phase 1: Quick Wins (Week 1-2)
- [ ] Implement embedding cache (40% embedding cost reduction)
- [ ] Enable prompt caching (90% input cost reduction on repeated prompts)
- [ ] Add low-value message filtering (30% processing reduction)
- [ ] Switch to batch API for consolidation jobs (50% discount)

**Expected impact:** 30-40% total cost reduction

### Phase 2: Model Optimization (Week 3-4)
- [ ] Implement complexity classifier
- [ ] Deploy model tiering (Haiku 3 / GPT-5 mini / Sonnet)
- [ ] Reduce embedding dimensions to 512-768
- [ ] Enable vector quantization

**Expected impact:** Additional 20-30% reduction

### Phase 3: Architecture (Week 5-8)
- [ ] Deploy Redis cache layer
- [ ] Implement tiered storage (hot/warm/cold)
- [ ] Set up cost monitoring dashboard
- [ ] Add per-user cost tracking

**Expected impact:** Additional 10-20% reduction

### Phase 4: Continuous Optimization (Ongoing)
- [ ] A/B test quality tradeoffs
- [ ] Monitor and adjust thresholds
- [ ] Evaluate self-hosting at scale
- [ ] Renegotiate volume pricing

---

## 7. Decision Framework

### When evaluating a cost optimization:

```
1. Estimate savings
   - What % of costs does this affect?
   - What's the absolute $ impact?

2. Assess quality impact
   - Does this affect user-visible behavior?
   - Can we measure the degradation?
   - Is it reversible?

3. Consider complexity
   - Implementation effort
   - Operational overhead
   - Debugging difficulty

4. Calculate ROI
   - savings_per_month / implementation_hours * hourly_rate
   - Break-even time

5. Prioritize
   - High savings + low quality impact + low complexity = DO FIRST
   - Any combination with high quality impact = CAREFUL TESTING
   - Low savings + high complexity = DEFER
```

### Cost-Quality Slider

```
Cost Priority ‚óÄ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚óè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ñ∂ Quality Priority

At "Cost Priority":
- Haiku 3 for all extraction
- 256-dim embeddings
- Aggressive consolidation
- 50% skip rate
- Cost: ~$0.002/memory

At "Quality Priority":
- Sonnet for all extraction  
- 1536-dim embeddings
- Preserve all granularity
- 10% skip rate
- Cost: ~$0.05/memory

Default (balanced):
- Model tiering
- 512-dim embeddings
- Moderate consolidation
- 35% skip rate
- Cost: ~$0.008/memory
```

---

## 8. Summary

### Top 5 Optimizations by Impact

| Rank | Optimization | Savings | Complexity |
|------|--------------|---------|------------|
| 1 | Model tiering | 40-60% | Medium |
| 2 | Caching (embedding + prompt) | 40-70% | Low |
| 3 | Batch processing | 30-50% | Low |
| 4 | Dimension reduction | 30-50% storage | Low |
| 5 | Importance filtering | 20-40% | Low |

### Target Metrics

| Metric | Naive System | Optimized System | Improvement |
|--------|--------------|------------------|-------------|
| Cost per memory | $0.05 | $0.008 | 84% reduction |
| Cost per retrieval | $0.005 | $0.0008 | 84% reduction |
| Storage per memory | 10KB | 2KB | 80% reduction |
| Monthly cost (1K users) | $7,000 | $1,500 | 79% reduction |

### Key Principles

1. **Cache everything** ‚Äî Repeat processing is pure waste
2. **Tier models by task** ‚Äî Save frontier models for frontier problems
3. **Batch when possible** ‚Äî 50% discounts are significant
4. **Skip the obvious** ‚Äî "ok" doesn't need extraction
5. **Compress storage** ‚Äî Dimensions and quantization compound
6. **Monitor relentlessly** ‚Äî Can't optimize what you don't measure

---

*Cost optimization is not about spending less‚Äîit's about spending wisely. The goal is maximum memory quality per dollar spent.*

‚Äî Jordan üß≠
